{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0adea0df5c75483ba0741f117fa32b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/217 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pes1ug22am170/.local/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b859cb23ba484209a3dc35943ce27a75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/676 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15080bfd4d8f4061b714d0f3fb1a7be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/448M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.07452493781844775\n",
      "Epoch 2/10, Loss: 0.010508173999066154\n",
      "Epoch 3/10, Loss: 0.002483786103160431\n",
      "Epoch 4/10, Loss: 0.0008832642827959111\n",
      "Epoch 5/10, Loss: 0.0004605450909972812\n",
      "Epoch 6/10, Loss: 0.00025182143387307104\n",
      "Epoch 7/10, Loss: 0.00015941930784417005\n",
      "Epoch 8/10, Loss: 0.00010426266453578137\n",
      "Epoch 9/10, Loss: 8.907409695287545e-05\n",
      "Epoch 10/10, Loss: 6.793791544623673e-05\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ViTMAEForPreTraining' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 156\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[1;32m    155\u001b[0m model_save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with your desired path\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m(model_save_path)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_metrics\u001b[39m(original, reconstructed):\n\u001b[1;32m    159\u001b[0m     original \u001b[38;5;241m=\u001b[39m original\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ViTMAEForPreTraining' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from transformers import ViTFeatureExtractor, ViTMAEForPreTraining\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torch.optim.lr_scheduler import StepLR, LambdaLR\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Initialize the feature extractor with do_rescale=False\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"facebook/vit-mae-base\")\n",
    "\n",
    "# Define paths and load model\n",
    "video_path = \"/home/pes1ug22am170/Masked_Vit/COSTA RICA IN 4K 60fps HDR (ULTRA HD).mp4\"  # Replace with the actual path to your video file\n",
    "model = ViTMAEForPreTraining.from_pretrained(\"facebook/vit-mae-base\")\n",
    "\n",
    "# Define hyperparameters\n",
    "batch_size = 4\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-5\n",
    "step_size = 5\n",
    "gamma = 0.1\n",
    "warmup_steps = 15\n",
    "\n",
    "# Function to show images\n",
    "imagenet_mean = np.array(feature_extractor.image_mean)\n",
    "imagenet_std = np.array(feature_extractor.image_std)\n",
    "\n",
    "def show_image(image, title=''):\n",
    "    assert image.shape[2] == 3\n",
    "    plt.imshow(torch.clip((image * imagenet_std + imagenet_mean) * 255, 0, 255).int())\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.axis('off')\n",
    "    return\n",
    "\n",
    "def visualize(pixel_values, model):\n",
    "    outputs = model(pixel_values)\n",
    "    y = model.unpatchify(outputs.logits)\n",
    "    y = torch.einsum('nchw->nhwc', y).detach().cpu()\n",
    "\n",
    "    mask = outputs.mask.detach()\n",
    "    mask = mask.unsqueeze(-1).repeat(1, 1, model.config.patch_size**2 * 3)\n",
    "    mask = model.unpatchify(mask)\n",
    "    mask = torch.einsum('nchw->nhwc', mask).detach().cpu()\n",
    "\n",
    "    x = torch.einsum('nchw->nhwc', pixel_values)\n",
    "    im_masked = x * (1 - mask)\n",
    "    im_paste = x * (1 - mask) + y * mask\n",
    "\n",
    "    plt.rcParams['figure.figsize'] = [24, 24]\n",
    "\n",
    "    plt.subplot(1, 4, 1)\n",
    "    show_image(x[0], \"original\")\n",
    "\n",
    "    plt.subplot(1, 4, 2)\n",
    "    show_image(im_masked[0], \"masked\")\n",
    "\n",
    "    plt.subplot(1, 4, 3)\n",
    "    show_image(y[0], \"reconstruction\")\n",
    "\n",
    "    plt.subplot(1, 4, 4)\n",
    "    show_image(im_paste[0], \"reconstruction + visible\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Function to extract frames from a video\n",
    "def extract_frames(video_path, frame_rate=1):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return []\n",
    "\n",
    "    frames = []\n",
    "    success, image = cap.read()\n",
    "    count = 0\n",
    "    while success:\n",
    "        if count % frame_rate == 0:\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(Image.fromarray(image_rgb))\n",
    "        success, image = cap.read()\n",
    "        count += 1\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "# Define custom dataset class\n",
    "class FrameDataset(Dataset):\n",
    "    def __init__(self, frames, feature_extractor):\n",
    "        self.frames = frames\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.RandomGrayscale(p=0.1),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frame = self.frames[idx]\n",
    "        frame = self.transform(frame)\n",
    "        # Ensure the frame is in the range [0, 1]\n",
    "        pixel_values = self.feature_extractor(images=frame, return_tensors=\"pt\").pixel_values.squeeze(0)\n",
    "        return pixel_values\n",
    "\n",
    "# Extract frames from the video\n",
    "frames = extract_frames(video_path, frame_rate=360)\n",
    "\n",
    "# Split frames into training and test sets\n",
    "train_frames, test_frames = train_test_split(frames, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = FrameDataset(train_frames, feature_extractor)\n",
    "test_dataset = FrameDataset(test_frames, feature_extractor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Optimizer, scheduler, and loss function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Define a learning rate scheduler with warmup\n",
    "def lr_lambda(current_step: int):\n",
    "    if current_step < warmup_steps:\n",
    "        return float(current_step) / float(max(1, warmup_steps))\n",
    "    return max(0.0, float(num_epochs * len(train_loader) - current_step) / float(max(1, num_epochs * len(train_loader) - warmup_steps)))\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch)\n",
    "        y = model.unpatchify(outputs.logits)\n",
    "        loss = criterion(y, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / len(train_loader)}\")\n",
    "\n",
    "# Save the model\n",
    "model_save_path = \"pred.h5\"  # Replace with your desired path\n",
    "model.save(model_save_path)\n",
    "\n",
    "def calculate_metrics(original, reconstructed):\n",
    "    original = original.permute(0, 2, 3, 1).cpu().numpy()\n",
    "    reconstructed = reconstructed.permute(0, 2, 3, 1).cpu().numpy()\n",
    "    data_range = original.max() - original.min()\n",
    "    psnr = np.mean([peak_signal_noise_ratio(o, r, data_range=data_range) for o, r in zip(original, reconstructed)])\n",
    "    \n",
    "    # Explicitly setting win_size and channel_axis for SSIM calculation\n",
    "    ssim = np.mean([\n",
    "        structural_similarity(o, r, data_range=data_range, multichannel=True, win_size=7, channel_axis=-1)\n",
    "        for o, r in zip(original, reconstructed)\n",
    "    ])\n",
    "    \n",
    "    return psnr, ssim\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "psnr_total = 0\n",
    "ssim_total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        outputs = model(batch)\n",
    "        y = model.unpatchify(outputs.logits)\n",
    "        loss = criterion(y, batch)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        psnr, ssim = calculate_metrics(batch, y)\n",
    "        psnr_total += psnr\n",
    "        ssim_total += ssim\n",
    "\n",
    "test_loss /= len(test_loader)\n",
    "psnr_total /= len(test_loader)\n",
    "ssim_total /= len(test_loader)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Average PSNR: {psnr_total}\")\n",
    "print(f\"Average SSIM: {ssim_total}\")\n",
    "\n",
    "# Visualize a few test samples\n",
    "for idx, frame in enumerate(test_frames[:5]):\n",
    "    pixel_values = feature_extractor(images=frame, return_tensors=\"pt\").pixel_values\n",
    "    visualize(pixel_values, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
